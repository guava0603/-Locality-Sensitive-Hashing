{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import re\n",
    "from binascii import crc32\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mappers\n",
    "\n",
    "一些常見的功能單一的 map function：\n",
    "\n",
    "  - #### mapper_expand_inner\n",
    "  把結構為 $[word, [docID_1, docID_2......]]$ 的資料轉為 $[(docID_1, [word]), (docID_2, [word])......]$（方便後續將很多個 y 結合進一個 list 中）\n",
    "\n",
    "  - #### mapper_switch\n",
    "  交換 pair 的 key 與 value\n",
    "  \n",
    "  - #### mapper_length\n",
    "  回傳 key 與 value list 的長度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_expand_inner(x):\n",
    "    return [(doc, [x[0]]) for doc in x[1]]\n",
    "def mapper_switch(x):\n",
    "    return (x[1], x[0])\n",
    "def mapper_length(x):\n",
    "    return (x[0], len(x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper 0\n",
    "\n",
    "\n",
    "讀入整段文章，先以空格切開不同字串，再用正規表達式去除掉無意義的標點符號（在字串最前或最後的標點符號），然後再回傳答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper0(line):\n",
    "    words = []\n",
    "    for word in line.split(' '):\n",
    "        Len = len(word)\n",
    "        if Len >= 1:\n",
    "            word = re.sub('[^a-zA-Z0-9]$', '', word)\n",
    "            word = re.sub('^[^a-zA-Z0-9]', '', word)\n",
    "            words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper 1\n",
    "\n",
    "input: $(word, idx)$<br>\n",
    "output: $[(idx, word), (idx, word), (idx, word)]$\n",
    "\n",
    "由於是 k shingle，將 k 個字串合稱為一個字段，則每個字串會在第 $idx - 2$, $idx - 1$, $idx$ 個字串時被使用到。<br>於是首先顛倒 key value 使字串有編號，而後回傳 k 個以利後續做到 k-shingle。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper1(x):\n",
    "    pair = []\n",
    "    for i in range(-k+1, 1):\n",
    "        if x[1]+i >= 0:\n",
    "            pair.append((x[1]+i, [x[0]]))\n",
    "    return pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper2\n",
    "\n",
    "input:  $(idx, [k\\_words])$<br>\n",
    "output: $(hash\\_value, [docID])$\n",
    "\n",
    "將 k 個字串以空白分隔後相連成一個長字串，接下來對字串做 encode，既保證 unique 又比較省空間。<br>\n",
    "為了避免文章末段的字串影響，因此排除掉長度不為 3 的情形。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper2(x, doc_id):\n",
    "    sep = ' '\n",
    "    return [(crc32(sep.join(x[1]).encode()), [doc_id])] if len(x[1]) == 3 else []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper3\n",
    "\n",
    "input:  $(docID, [shingleIDs......])$<br>\n",
    "output: $[(hashID, docID), value)......]$\n",
    "\n",
    "以 100 個 hash function 來達到 min-hashing 的目標，在這邊的 hash function 是設計成：<br>\n",
    "$= (i * (docID+1)) \\% P$<br>\n",
    "其中 i 是 0~100 的變數，idx 是字段編號，P 是自訂的一個質數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper3(x):\n",
    "    hash_value = []\n",
    "    for i in range(100):\n",
    "        min_val = (i*x[1][0]) % P % N\n",
    "        for sig_id in x[1]:\n",
    "            new_min_val = (i*sig_id) % P % N\n",
    "            if new_min_val < min_val:\n",
    "                min_val = new_min_val\n",
    "        hash_value.append(((i, x[0]), min_val))\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper4\n",
    "\n",
    "input:  $((hashID, docID), minhash))$<br>\n",
    "output: $((bandID, docID), bucketID))$\n",
    "\n",
    "因為每個 document 會有 100 個 hash function，為了限縮為 50 個 band，因此用 hash function ID 除以 2 來得到 band 的 ID。<br>\n",
    "接著對於 row 0 的資料乘以 2，對於 row 1 的資料乘以 5，以此來做 hashing。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper4(x):\n",
    "    if x[0][0] % 2 == 0:\n",
    "        return ((round(x[0][0] / 2), x[0][1]), x[1])\n",
    "    else:\n",
    "        return ((round(x[0][0] / 2), x[0][1]), x[1] * N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper5\n",
    "\n",
    "input:  $((bandID, docID), bucketID))$<br>\n",
    "output: $((bandID, bucketID), [docID])$\n",
    "\n",
    "由於要看的是哪些 document 的同一個 band 會 hash 到同一個 bucket 當中，因此把 band 與 hash function 的 ID 當作 key，再把 document 的 ID 作為 list 回傳。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper5(x):\n",
    "    return ((x[0][0], x[1]), [x[0][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper6\n",
    "\n",
    "input:  $((bandID, bucketID), [docID_1, docID_2......]))$<br>\n",
    "output: $[(docID_1, docID_2), (docID_1, docID_3)......]$\n",
    "\n",
    "用來選出 candidate pair 的 mapper，只要出現在同一個 band 與 bucket 中的 document，就是可能有高相似度的 document。<br>\n",
    "因此可以忽略 key，只針對 sort 過的 value list，做成由小至大的 pair 後一併回傳。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper6(x):\n",
    "    cand_pair = []\n",
    "    len1 = len(x[1])\n",
    "    arr = sorted(x[1])\n",
    "    for i in range(len1):\n",
    "        for j in range(i+1, len1):\n",
    "            cand_pair.append((arr[i], arr[j]))\n",
    "    return cand_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper7\n",
    "\n",
    "input:  $(shingleID, [docID_1, docID_2......]))$<br>\n",
    "output: $[((docID_1, docID_2), 1), ((docID_1, docID_3), 1)......]$\n",
    "\n",
    "用來找出所有在某個 shingle 下重複的 document，作法與 mapper6 相似，只是為了統計總數而多加了一個 count value。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper7(x):\n",
    "    List = []\n",
    "    len1 = len(x[1])\n",
    "    arr = sorted(x[1])\n",
    "    \n",
    "    for i in range(len1):\n",
    "        for j in range(i+1, len1):\n",
    "            List.append(((arr[i], arr[j]), 1))\n",
    "    return List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reducer0\n",
    "\n",
    "最常見的 reducer，將 value 值加總"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer0(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reducer 1\n",
    "\n",
    "input: $((hashID, docID), value))$\n",
    "\n",
    "用來找出最小值的 reducer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer1(x, y):\n",
    "    return y if y < x else x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting\n",
    "\n",
    "Create Spark context，以及設定好固定的變數。<br>\n",
    "$k$ 為題目規定的 shingle 值。<br>\n",
    "$doc\\_num$ 為文章總數。<br>\n",
    "$P$ 為 Hash function 用的質數。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "k = 3\n",
    "doc_num = 101\n",
    "P = 29989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input && K-Shingling\n",
    "\n",
    "讀入 $doc\\_num$ 篇文章的資料。<br>\n",
    "以 $shingle\\_doc\\_map$ 儲存 shingleID 與所有相關 docIDs 的對應表。<br>\n",
    "以 $shingle\\_set$ 儲存 docID 與其擁有的 shingleIDs 的對應表。<br>\n",
    "以 $N$ 儲存 shingles 總數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "shingle_doc_map = sc.parallelize(list())\n",
    "\n",
    "for i in range(doc_num):\n",
    "    each_word_in_doc = sc.textFile(\"athletics/%03d.txt\"%(i+1)).flatMap(mapper0)\n",
    "    k_words_in_doc = each_word_in_doc.zipWithIndex().flatMap(mapper1).reduceByKey(reducer0)\n",
    "    k_words_in_doc_with_id = k_words_in_doc.flatMap(lambda x: mapper2(x, i))\n",
    "    shingle_doc_map = shingle_doc_map.union(k_words_in_doc_with_id)\n",
    "    \n",
    "shingle_doc_map = shingle_doc_map.reduceByKey(reducer0).zipWithIndex()\n",
    "shingle_doc_map = shingle_doc_map.map(lambda x: (x[1], list(set(x[0][1]))))\n",
    "shingle_set = shingle_doc_map.flatMap(mapper_expand_inner).reduceByKey(reducer0)\n",
    "\n",
    "N = shingle_doc_map.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Hashing\n",
    "\n",
    "以 $minhash\\_set$ 儲存 100 個 hash function 針對 101 篇文章與 N 個 shingles 計算後的 min hashing 結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26713\n"
     ]
    }
   ],
   "source": [
    "minhash_set = shingle_set.flatMap(mapper3).reduceByKey(reducer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH\n",
    "\n",
    "針對 minhash 後的結果將它們 hash 到 bucket 之上，儲存為 $bucket\\_set$。<br>\n",
    "將 $bucket\\_set$ 中所有曾被 hash 到一起的文章兩兩取出，作為 $sim\\_doc\\_set$。<br>\n",
    "最後做 distinct 得出完整的 $candidate\\_pair\\_set$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673\n"
     ]
    }
   ],
   "source": [
    "bucket_set = minhash_set.map(mapper4).reduceByKey(reducer0)\n",
    "sim_doc_set = bucket_set.map(mapper5).reduceByKey(reducer0)\n",
    "candidate_pair_set = sim_doc_set.flatMap(mapper6).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Distance\n",
    "\n",
    "首先得出每篇 document 的 shingle 數量，存入 $doc\\_len\\_set$ 之中。<br>\n",
    "接著透過 $shingle\\_doc\\_map$ 得到所有共享同一個字段的 document 的 ID，每共享一個就會出現一次，加總和後以 $join\\_set$ 來存放所有文件彼此重複的字段的數量。<br>\n",
    "接下來，依序使每對 candidate pair 的兩篇文章都透過 join 來得到對應長度；接著再針對整個 pair，透過 join 獲取 pair 重複字段的數量，再以 map 來根據這些資料取得各個 pair 的 jaccard distance。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len_set = shingle_set.map(mapper_length)\n",
    "join_set = shingle_doc_map.flatMap(mapper7).reduceByKey(reducer0)\n",
    "ans = candidate_pair_set.join(doc_len_set).map(lambda x: (x[1][0], (x[0], x[1][1]))).join(doc_len_set)\n",
    "ans = ans.map(lambda x: ((x[1][0][0], x[0]), x[1][0][1] + x[1][1])).join(join_set).map(lambda x: (x[0], x[1][1] / (x[1][0] - x[1][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "將順序倒過來再 sortByKey 後依序輸出即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(012, 020) : 100.00 %\n",
      "(052, 084) : 100.00 %\n",
      "(047, 049) : 75.50 %\n",
      "(030, 035) : 70.67 %\n",
      "(049, 088) : 50.98 %\n",
      "(048, 049) : 48.49 %\n",
      "(023, 038) : 48.21 %\n",
      "(014, 040) : 39.74 %\n",
      "(047, 088) : 38.55 %\n",
      "(047, 048) : 36.61 %\n"
     ]
    }
   ],
   "source": [
    "sorted_ans = ans.map(mapper_switch).sortByKey(ascending=False).take(10)\n",
    "for doc in sorted_ans:\n",
    "    print(\"(%03d, %03d) : %.2f %%\" % (doc[1][0]+1, doc[1][1]+1, doc[0]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
